# -*- coding: utf-8 -*-
"""Working Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vA8TqOLErXDUWfRgb64jlfhAfe_n_1kN

#Chatbot

A chatbot is a conversational application that aids in customer service, engagement, and support by replacing or augmenting human support agents with artificial intelligence (AI) and other automation technologies that can communicate with end-users via chat.

#Dataset

I made my own data, which is a json file that contains:

*   Tag:The subject of the text
*   Patterns: the questions that would be the input
*   responses: the answers on the questions, and it is our target

#Import Libraries

###Install tflearn package
"""

#!pip install tflearn

import pandas as pd
import numpy as np
import nltk
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
from nltk.stem.lancaster import LancasterStemmer 
stemmer = LancasterStemmer()
lemmatizer = WordNetLemmatizer()

import tflearn
import tensorflow 
import random

import json
import pickle

from keras.layers.core.embedding import Embedding
from tensorflow.keras.layers import RepeatVector, LSTM, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.models import Sequential
from keras.optimizers import SGD

"""#Loading the Data"""

with open("intents.json") as f:
  data = json.load(f)

"""#Data Extraction

To retrieve the information from the json file, we need some empty lists to split the data and add them into this lists
"""

words = []
labels = []
docs_x = []
docs_y = []
documents = []

for intent in data['intents']:
  for pattern in intent['patterns']:
    word_list = nltk.word_tokenize(pattern)
    words.extend(word_list)
    docs_x.append(word_list)
    docs_y.append(intent['tag'])
    # associating patterns with respective tags
    documents.append(((word_list), intent['tag']))
  if intent['tag'] not in labels:
    labels.append(intent['tag'])

words

labels

docs_x

docs_y

documents

"""###Saving words and labels

#Word stemming

Finding the root of a word is known as stemming. For instance, the stem of the word “thats” stem might be “that,” whereas the stem of the word “happening” could be “happen.”
"""

words = [stemmer.stem(c.lower()) for c in words if c not in ["?","!",".",","]]
words = sorted(list(set(words)))

labels = sorted(labels)

# saving the words and classes list to binary files
pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(labels, open('labels.pkl', 'wb'))

words

labels

"""#Bag of words

It’s time to speak about a bag of words now that we’ve imported our data and generated a stemmed vocabulary. Neural networks and machine learning algorithms, as we all know, require numerical input. So our string list isn’t going to cut it. We need a mechanism to represent numbers in our sentences, which is where a bag of words comes in.
"""

# training = []
# output = []

# out_empty = [0 for _ in range(len(labels))] #to prepare the output data and pad it

# #1 if the data is present in the data and 0 otherwise
# for x, doc in enumerate(docs_x):
#   bag= []

#   w = [stemmer.stem(c.lower()) for c in doc]

#   for c in words:
#     if c in w:
#       bag.append(1)
#     else:
#       bag.append(0)
#   output_row = out_empty[:]
#   output_row[labels.index(docs_y[x])] = 1

#   training.append(bag)
#   output.append(output_row)

"""or"""

# we need numerical values of the
# words because a neural network
# needs numerical values to work with
training = []
output_empty = [0]*len(labels)
for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [stemmer.stem(c.lower()) for c in word_patterns if c not in ["?","!",".",","]]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
          
    # making a copy of the output_empty
    output_row = list(output_empty)
    output_row[labels.index(document[1])] = 1
    training.append([bag, output_row])
random.shuffle(training)
training = np.array(training)
  
# splitting the data
train_x = list(training[:, 0])
train_y = list(training[:, 1])

nltk.download('wordnet')
nltk.download('omw-1.4')

documents

documents[0]

bag

"""###Coverting the training and output data to arrays"""

# training=np.array(training)
# output = np.array(output)

training

# print(training.shape)
# print(output.shape)

"""#Model Development

##Build the Model
"""

# we need numerical values of the
# words because a neural network
# needs numerical values to work with
training = []
output_empty = [0]*len(labels)
for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [stemmer.stem(c.lower()) for c in word_patterns if c not in ["?","!",".",","]]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
          
    # making a copy of the output_empty
    output_row = list(output_empty)
    output_row[labels.index(document[1])] = 1
    training.append([bag, output_row])
random.shuffle(training)
training = np.array(training)
  
# splitting the data
train_x = list(training[:, 0])
train_y = list(training[:, 1])

tensorflow.compat.v1.reset_default_graph()

x = np.array(train_x)
y = np.array(train_y)

print(x.shape)
print(y.shape)

net = tflearn.input_data(shape=[None,len(x[0])])
net = tflearn.fully_connected(net,8)
net = tflearn.fully_connected(net,8)
net = tflearn.fully_connected(net,len(y[0]),activation="softmax")
net = tflearn.regression(net) # this layer contains the optimizer which is adam by default

tflearmmodel = tflearn.DNN(net)

"""##Model Training"""

tflearmmodel.fit(x,y,n_epoch=500, batch_size=5,show_metric=True)

"""###Saving the Model"""

tflearmmodel.save("tflearmmodel.tflearn")

"""#Building and Testing the ChatBot"""

def clean_up_sentences(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
    return sentence_words

def bagw(sentence):
    sentence_words = clean_up_sentences(sentence)
    bag = [0]*len(words)
    for w in sentence_words:
        for i, word in enumerate(words):
            if word == w:
                bag[i] = 1
    return np.array(bag)

# def predict_class(sentence):
#     bow = bagw(sentence)
#     res = model.predict(np.array([bow]))[0]
#     ERROR_THRESHOLD = 0.25
#     results = [[i, r] for i, r in enumerate(res)
#                if r > ERROR_THRESHOLD]
#     results.sort(key=lambda x: x[1], reverse=True)
#     return_list = []
#     for r in results:
#         return_list.append({'intent': labels[r[0]],
#                             'probability': str(r[1])})
#         return return_list



def chat(model):
  print("Welcome to ChatBot!")
  while True:
    input_text = input("Me: ")
    if input_text.lower() == "bye":
      break
    
    prediction = model.predict([bagw(input_text)])
    results_index = np.argmax(prediction)
    tag = labels[results_index]

    for tg in data["intents"]:
      if tg["tag"] == tag:
        responses = tg["responses"]
    
    #to generate random responses from the data
    print(random.choice(responses))

chat(tflearmmodel)

"""I found this model not usefull a lot, so I will using another model using keras library

#Model 2

Same steps as before, with little edits on the code, but it is the same idea and work
"""

# we need numerical values of the
# words because a neural network
# needs numerical values to work with
training = []
output_empty = [0]*len(labels)
for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [stemmer.stem(c.lower()) for c in word_patterns if c not in ["?","!",".",","]]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
          
    # making a copy of the output_empty
    output_row = list(output_empty)
    output_row[labels.index(document[1])] = 1
    training.append([bag, output_row])
random.shuffle(training)
training = np.array(training)
  
# splitting the data
train_x = list(training[:, 0])
train_y = list(training[:, 1])

training.shape

#output.shape

training

train_y

"""##Build a model"""

import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]), ),
                activation='relu'))
model.add(Dropout(0.5))
#model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), 
                activation='softmax'))

model.summary()

"""##compile the model"""

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd, metrics=['accuracy'])

np.array(train_x).shape

np.array(train_y).shape

"""##train the model"""

hist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=5, verbose=1)

"""##save the model to use it in deployment"""

model.save("chatbotmodel.h5",hist)

"""###load the words and lables"""

from keras.models import load_model
words = pickle.load(open('words.pkl', 'rb'))
labels = pickle.load(open('labels.pkl', 'rb'))
#model = load_model('chatbotmodel.h5')

len(words)

"""##prediction"""

def clean_up_sentences(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
    return sentence_words

def bagw(sentence):
    sentence_words = clean_up_sentences(sentence)
    bag = [0]*len(words)
    for w in sentence_words:
        for i, word in enumerate(words):
            if word == w:
                bag[i] = 1
    return np.array(bag)

def predict_class(sentence):
    bow = bagw(sentence)
    res = model.predict(np.array([bow]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res)
               if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({'intent': labels[r[0]],
                            'probability': str(r[1])})
        return return_list

def get_response(intents_list, intents_json):
    tag = intents_list[0]['intent']
    list_of_intents = intents_json['intents']
    result = ""
    for i in list_of_intents:
        if i['tag'] == tag:
            result = random.choice(i['responses'])
            #break
    return result

"""##Testing"""

print("Welcome to ChatBot!")
while True:
    message = input("You: ")
    ints = predict_class(message)
    res = get_response(ints, data)
    print("Bot: " + res)

    if message.lower() == "bye":
      break